{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0Pz6qNnYWO7gQWibv1BaV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raki-rankawat/stm32/blob/main/VWW_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import tarfile\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "XKrw7vV6YYI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d1Medmla-18",
        "outputId": "b6c16e54-b33c-4c53-bf12-26a0b3cd4da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Auto Download + Prepare VWW (10k subset)\n",
        "# ----------------------------\n",
        "vww_url = \"https://www.silabs.com/public/files/github/machine_learning/benchmarks/datasets/vw_coco2014_96.tar.gz\"\n",
        "\n",
        "base_dir = Path(\"/content/vww_work\")\n",
        "archive_path = base_dir / \"vw_coco2014_96.tar.gz\"\n",
        "extract_dir = base_dir / \"extracted\"\n",
        "subset_dir = base_dir / \"vww_10k\"\n",
        "\n",
        "# Subset config: 5k person + 5k non_person\n",
        "n_per_class = 5000\n",
        "val_ratio = 0.20\n",
        "\n",
        "random.seed(41)\n",
        "torch.manual_seed(41)\n",
        "\n",
        "def download_vww():\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if archive_path.exists() and archive_path.stat().st_size > 0:\n",
        "        print(\"‚úÖ VWW archive already downloaded\")\n",
        "        return\n",
        "\n",
        "    print(\"‚¨áÔ∏è Downloading VWW archive...\")\n",
        "    urlretrieve(vww_url, archive_path)\n",
        "    print(\"‚úÖ Download complete:\", archive_path)\n",
        "\n",
        "def extract_vww():\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if any(extract_dir.iterdir()):\n",
        "        print(\"‚úÖ VWW already extracted\")\n",
        "        return\n",
        "\n",
        "    print(\"üì¶ Extracting VWW archive...\")\n",
        "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
        "        tar.extractall(extract_dir)\n",
        "    print(\"‚úÖ Extraction complete:\", extract_dir)\n",
        "\n",
        "def find_vww_root():\n",
        "    # Find folder that contains BOTH person/ and non_person/\n",
        "    for p in extract_dir.rglob(\"person\"):\n",
        "        if p.is_dir() and (p.parent / \"non_person\").is_dir():\n",
        "            return p.parent\n",
        "    raise RuntimeError(\"‚ùå Could not find 'person' and 'non_person' directories under extracted dataset\")\n",
        "\n",
        "def list_images(folder):\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\"}\n",
        "    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
        "\n",
        "def make_vww_subset(src_root):\n",
        "    # Skip if subset already exists\n",
        "    if (subset_dir / \"train\" / \"person\").is_dir() and (subset_dir / \"val\" / \"non_person\").is_dir():\n",
        "        print(\"‚úÖ VWW 10k subset already exists:\", subset_dir)\n",
        "        return\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        for c in [\"person\", \"non_person\"]:\n",
        "            (subset_dir / split / c).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    person_imgs = list_images(src_root / \"person\")\n",
        "    nonperson_imgs = list_images(src_root / \"non_person\")\n",
        "\n",
        "    if len(person_imgs) < n_per_class or len(nonperson_imgs) < n_per_class:\n",
        "        raise ValueError(\n",
        "            f\"‚ùå Not enough images:\\n\"\n",
        "            f\"person: {len(person_imgs)} (need {n_per_class})\\n\"\n",
        "            f\"non_person: {len(nonperson_imgs)} (need {n_per_class})\"\n",
        "        )\n",
        "\n",
        "    random.shuffle(person_imgs)\n",
        "    random.shuffle(nonperson_imgs)\n",
        "\n",
        "    person_sel = person_imgs[:n_per_class]\n",
        "    nonperson_sel = nonperson_imgs[:n_per_class]\n",
        "\n",
        "    def split_list(lst, val_ratio):\n",
        "        n_val = int(len(lst) * val_ratio)\n",
        "        return lst[n_val:], lst[:n_val]  # train, val\n",
        "\n",
        "    p_train, p_val = split_list(person_sel, val_ratio)\n",
        "    n_train, n_val = split_list(nonperson_sel, val_ratio)\n",
        "\n",
        "    def copy_files(files, dst_dir):\n",
        "        for f in files:\n",
        "            dst = dst_dir / f.name\n",
        "            # avoid rare collisions\n",
        "            if dst.exists():\n",
        "                dst = dst_dir / (f\"{f.parent.name}_{f.name}\")\n",
        "            shutil.copy2(f, dst)\n",
        "\n",
        "    print(\"üß© Creating VWW 10k subset...\")\n",
        "    copy_files(p_train, subset_dir / \"train\" / \"person\")\n",
        "    copy_files(p_val,   subset_dir / \"val\"   / \"person\")\n",
        "    copy_files(n_train, subset_dir / \"train\" / \"non_person\")\n",
        "    copy_files(n_val,   subset_dir / \"val\"   / \"non_person\")\n",
        "    print(\"‚úÖ VWW subset created at:\", subset_dir)\n",
        "\n",
        "download_vww()\n",
        "extract_vww()\n",
        "vww_root = find_vww_root()\n",
        "print(\"‚úÖ Found VWW root:\", vww_root)\n",
        "make_vww_subset(vww_root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx9VX0uXY7fa",
        "outputId": "c4032e84-f5bd-40e1-c07a-b5761be88e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading VWW archive...\n",
            "‚úÖ Download complete: /content/vww_work/vw_coco2014_96.tar.gz\n",
            "üì¶ Extracting VWW archive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4249398104.py:38: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(extract_dir)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extraction complete: /content/vww_work/extracted\n",
            "‚úÖ Found VWW root: /content/vww_work/extracted/vw_coco2014_96\n",
            "üß© Creating VWW 10k subset...\n",
            "‚úÖ VWW subset created at: /content/vww_work/vww_10k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Data Loaders (same style)\n",
        "# ----------------------------\n",
        "batch_size = 64\n",
        "img_size = 96\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(img_size, scale=(0.6, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=str(subset_dir / \"train\"), transform=train_transform)\n",
        "test_data  = datasets.ImageFolder(root=str(subset_dir / \"val\"),   transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Class mapping:\", train_data.class_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDoK0FE5Y2dL",
        "outputId": "108b24d7-e520-4131-d9a5-f1f0a21de136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class mapping: {'non_person': 0, 'person': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# CNN Model (same style) - FINAL\n",
        "# ----------------------------\n",
        "class VWWConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # For 96x96 with 4 pools: 96->48->24->12->6\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "OlsIIY59YzAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Random Seeds and Model Instance (same style)\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(41)\n",
        "model = VWWConvNet().to(device)"
      ],
      "metadata": {
        "id": "S_kMARXuYuw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Loss & Optimizer (same style) - FINAL\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "FquLpcNNYtXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Training (same style)\n",
        "# ----------------------------\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train() # training mode\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(X)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad(set_to_none = True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        batch_size = y.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        preds = outputs.argmax(dim = 1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += batch_size\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "ejSwpoccYrqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Testing (same style)\n",
        "# ----------------------------\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval() # testing mode\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Statistics\n",
        "            batch_size = y.size(0)\n",
        "            running_loss += loss.item() * batch_size\n",
        "            preds = outputs.argmax(dim = 1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += batch_size\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "gQuNVqf6YhcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Run (same style print) - FINAL (best checkpoint + early stopping)\n",
        "# ----------------------------\n",
        "epochs = 50\n",
        "start_time = time.time()\n",
        "\n",
        "best_acc = 0.0\n",
        "best_epoch = 1\n",
        "patience = 8\n",
        "patience_counter = 0\n",
        "\n",
        "best_path = \"/content/drive/My Drive/Colab Notebooks/stm_vww_best.pth\"\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch}/{epochs} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc * 100:.2f}% | \"\n",
        "        f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc * 100:.2f}%\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(\"‚úÖ Model saved as stm_vww_best.pth for epoch\", best_epoch)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"üõë Early stopping at epoch\", epoch, \"| Best epoch:\", best_epoch, \"| Best Test Acc:\", f\"{best_acc*100:.2f}%\")\n",
        "        break\n",
        "\n",
        "print(f\"Training Time: {(time.time() - start_time) / 60} minutes!\")\n",
        "print(\"Best epoch:\", best_epoch, \"| Best Test Acc:\", f\"{best_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJE8_D_lYeQU",
        "outputId": "a2095191-fc65-4953-a9e8-8c8e7888990d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50 | Train Loss: 0.6790 | Train Acc: 60.89% | Test Loss: 0.6175 | Test Acc: 66.10%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 1\n",
            "Epoch: 2/50 | Train Loss: 0.6309 | Train Acc: 65.03% | Test Loss: 0.6353 | Test Acc: 62.60%\n",
            "Epoch: 3/50 | Train Loss: 0.6075 | Train Acc: 67.74% | Test Loss: 0.5751 | Test Acc: 69.50%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 3\n",
            "Epoch: 4/50 | Train Loss: 0.5854 | Train Acc: 68.75% | Test Loss: 0.6575 | Test Acc: 60.25%\n",
            "Epoch: 5/50 | Train Loss: 0.5795 | Train Acc: 69.91% | Test Loss: 0.5508 | Test Acc: 73.15%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 5\n",
            "Epoch: 6/50 | Train Loss: 0.5667 | Train Acc: 70.99% | Test Loss: 0.5605 | Test Acc: 71.80%\n",
            "Epoch: 7/50 | Train Loss: 0.5560 | Train Acc: 72.32% | Test Loss: 0.5762 | Test Acc: 71.90%\n",
            "Epoch: 8/50 | Train Loss: 0.5471 | Train Acc: 73.08% | Test Loss: 0.5462 | Test Acc: 73.85%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 8\n",
            "Epoch: 9/50 | Train Loss: 0.5418 | Train Acc: 72.81% | Test Loss: 0.6337 | Test Acc: 67.10%\n",
            "Epoch: 10/50 | Train Loss: 0.5295 | Train Acc: 73.67% | Test Loss: 0.5413 | Test Acc: 73.30%\n",
            "Epoch: 11/50 | Train Loss: 0.5274 | Train Acc: 74.12% | Test Loss: 0.5246 | Test Acc: 74.55%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 11\n",
            "Epoch: 12/50 | Train Loss: 0.5199 | Train Acc: 74.54% | Test Loss: 0.5122 | Test Acc: 75.40%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 12\n",
            "Epoch: 13/50 | Train Loss: 0.5142 | Train Acc: 75.30% | Test Loss: 0.7034 | Test Acc: 66.25%\n",
            "Epoch: 14/50 | Train Loss: 0.5067 | Train Acc: 75.98% | Test Loss: 0.7616 | Test Acc: 60.90%\n",
            "Epoch: 15/50 | Train Loss: 0.5100 | Train Acc: 75.96% | Test Loss: 0.5155 | Test Acc: 75.35%\n",
            "Epoch: 16/50 | Train Loss: 0.4915 | Train Acc: 76.46% | Test Loss: 0.5651 | Test Acc: 70.00%\n",
            "Epoch: 17/50 | Train Loss: 0.4891 | Train Acc: 77.36% | Test Loss: 0.5029 | Test Acc: 76.05%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 17\n",
            "Epoch: 18/50 | Train Loss: 0.4925 | Train Acc: 76.25% | Test Loss: 0.6651 | Test Acc: 68.75%\n",
            "Epoch: 19/50 | Train Loss: 0.4835 | Train Acc: 77.05% | Test Loss: 0.5688 | Test Acc: 70.20%\n",
            "Epoch: 20/50 | Train Loss: 0.4783 | Train Acc: 76.96% | Test Loss: 0.5560 | Test Acc: 73.40%\n",
            "Epoch: 21/50 | Train Loss: 0.4711 | Train Acc: 77.51% | Test Loss: 0.5352 | Test Acc: 73.95%\n",
            "Epoch: 22/50 | Train Loss: 0.4696 | Train Acc: 78.06% | Test Loss: 0.6526 | Test Acc: 70.15%\n",
            "Epoch: 23/50 | Train Loss: 0.4664 | Train Acc: 77.98% | Test Loss: 0.5392 | Test Acc: 74.75%\n",
            "Epoch: 24/50 | Train Loss: 0.4628 | Train Acc: 78.38% | Test Loss: 0.4913 | Test Acc: 76.25%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 24\n",
            "Epoch: 25/50 | Train Loss: 0.4572 | Train Acc: 78.01% | Test Loss: 0.6913 | Test Acc: 68.95%\n",
            "Epoch: 26/50 | Train Loss: 0.4539 | Train Acc: 78.59% | Test Loss: 0.6536 | Test Acc: 69.90%\n",
            "Epoch: 27/50 | Train Loss: 0.4518 | Train Acc: 79.25% | Test Loss: 0.4772 | Test Acc: 77.40%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 27\n",
            "Epoch: 28/50 | Train Loss: 0.4405 | Train Acc: 79.57% | Test Loss: 0.4815 | Test Acc: 77.25%\n",
            "Epoch: 29/50 | Train Loss: 0.4427 | Train Acc: 79.42% | Test Loss: 0.5192 | Test Acc: 75.30%\n",
            "Epoch: 30/50 | Train Loss: 0.4315 | Train Acc: 79.94% | Test Loss: 0.6225 | Test Acc: 69.70%\n",
            "Epoch: 31/50 | Train Loss: 0.4309 | Train Acc: 80.11% | Test Loss: 0.4786 | Test Acc: 78.45%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 31\n",
            "Epoch: 32/50 | Train Loss: 0.4307 | Train Acc: 80.19% | Test Loss: 0.4760 | Test Acc: 78.00%\n",
            "Epoch: 33/50 | Train Loss: 0.4288 | Train Acc: 80.03% | Test Loss: 0.5083 | Test Acc: 76.85%\n",
            "Epoch: 34/50 | Train Loss: 0.4190 | Train Acc: 80.77% | Test Loss: 0.5132 | Test Acc: 74.85%\n",
            "Epoch: 35/50 | Train Loss: 0.4219 | Train Acc: 80.71% | Test Loss: 0.4658 | Test Acc: 77.75%\n",
            "Epoch: 36/50 | Train Loss: 0.4178 | Train Acc: 80.62% | Test Loss: 0.4720 | Test Acc: 78.35%\n",
            "Epoch: 37/50 | Train Loss: 0.4153 | Train Acc: 80.77% | Test Loss: 0.4793 | Test Acc: 77.10%\n",
            "Epoch: 38/50 | Train Loss: 0.4129 | Train Acc: 81.10% | Test Loss: 0.4622 | Test Acc: 79.00%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 38\n",
            "Epoch: 39/50 | Train Loss: 0.4146 | Train Acc: 81.20% | Test Loss: 0.5414 | Test Acc: 75.45%\n",
            "Epoch: 40/50 | Train Loss: 0.4031 | Train Acc: 81.75% | Test Loss: 0.6225 | Test Acc: 73.80%\n",
            "Epoch: 41/50 | Train Loss: 0.4100 | Train Acc: 81.40% | Test Loss: 0.4614 | Test Acc: 78.50%\n",
            "Epoch: 42/50 | Train Loss: 0.4008 | Train Acc: 81.40% | Test Loss: 0.4733 | Test Acc: 78.70%\n",
            "Epoch: 43/50 | Train Loss: 0.3974 | Train Acc: 81.55% | Test Loss: 0.5045 | Test Acc: 77.85%\n",
            "Epoch: 44/50 | Train Loss: 0.3981 | Train Acc: 81.81% | Test Loss: 0.4676 | Test Acc: 78.70%\n",
            "Epoch: 45/50 | Train Loss: 0.3877 | Train Acc: 82.78% | Test Loss: 0.4898 | Test Acc: 78.00%\n",
            "Epoch: 46/50 | Train Loss: 0.3888 | Train Acc: 82.19% | Test Loss: 0.5239 | Test Acc: 77.55%\n",
            "üõë Early stopping at epoch 46 | Best epoch: 38 | Best Test Acc: 79.00%\n",
            "Training Time: 101.09534797668456 minutes!\n",
            "Best epoch: 38 | Best Test Acc: 79.00%\n"
          ]
        }
      ]
    }
  ]
}