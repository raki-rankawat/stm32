{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcM9sfsbhK8bmiLgkKaHbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raki-rankawat/stm32/blob/main/VWW_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained Model (VWW - same style as CIFAR-10)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import tarfile\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "XKrw7vV6YYI4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d1Medmla-18",
        "outputId": "6b3c3837-38e3-4345-98fd-ad2861c3cf4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Auto Download + Prepare VWW (10k subset)\n",
        "# ----------------------------\n",
        "# VWW preprocessed dataset (96x96) download URL\n",
        "vww_url = \"https://www.silabs.com/public/files/github/machine_learning/benchmarks/datasets/vw_coco2014_96.tar.gz\"\n",
        "\n",
        "base_dir = Path(\"/content/vww_work\")\n",
        "archive_path = base_dir / \"vw_coco2014_96.tar.gz\"\n",
        "extract_dir = base_dir / \"extracted\"\n",
        "subset_dir = base_dir / \"vww_10k\"\n",
        "\n",
        "# Subset config: 5k person + 5k non_person\n",
        "n_per_class = 5000\n",
        "val_ratio = 0.20\n",
        "\n",
        "random.seed(41)\n",
        "torch.manual_seed(41)\n",
        "\n",
        "def download_vww():\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if archive_path.exists() and archive_path.stat().st_size > 0:\n",
        "        print(\"‚úÖ VWW archive already downloaded\")\n",
        "        return\n",
        "\n",
        "    print(\"‚¨áÔ∏è Downloading VWW archive...\")\n",
        "    urlretrieve(vww_url, archive_path)\n",
        "    print(\"‚úÖ Download complete:\", archive_path)\n",
        "\n",
        "def extract_vww():\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if any(extract_dir.iterdir()):\n",
        "        print(\"‚úÖ VWW already extracted\")\n",
        "        return\n",
        "\n",
        "    print(\"üì¶ Extracting VWW archive...\")\n",
        "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
        "        tar.extractall(extract_dir)\n",
        "    print(\"‚úÖ Extraction complete:\", extract_dir)\n",
        "\n",
        "def find_vww_root():\n",
        "    # Find folder that contains BOTH person/ and non_person/\n",
        "    for p in extract_dir.rglob(\"person\"):\n",
        "        if p.is_dir() and (p.parent / \"non_person\").is_dir():\n",
        "            return p.parent\n",
        "    raise RuntimeError(\"‚ùå Could not find 'person' and 'non_person' directories under extracted dataset\")\n",
        "\n",
        "def list_images(folder):\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\"}\n",
        "    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
        "\n",
        "def make_vww_subset(src_root):\n",
        "    # Skip if subset already exists\n",
        "    if (subset_dir / \"train\" / \"person\").is_dir() and (subset_dir / \"val\" / \"non_person\").is_dir():\n",
        "        print(\"‚úÖ VWW 10k subset already exists:\", subset_dir)\n",
        "        return\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        for c in [\"person\", \"non_person\"]:\n",
        "            (subset_dir / split / c).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    person_imgs = list_images(src_root / \"person\")\n",
        "    nonperson_imgs = list_images(src_root / \"non_person\")\n",
        "\n",
        "    if len(person_imgs) < n_per_class or len(nonperson_imgs) < n_per_class:\n",
        "        raise ValueError(\n",
        "            f\"‚ùå Not enough images:\\n\"\n",
        "            f\"person: {len(person_imgs)} (need {n_per_class})\\n\"\n",
        "            f\"non_person: {len(nonperson_imgs)} (need {n_per_class})\"\n",
        "        )\n",
        "\n",
        "    random.shuffle(person_imgs)\n",
        "    random.shuffle(nonperson_imgs)\n",
        "\n",
        "    person_sel = person_imgs[:n_per_class]\n",
        "    nonperson_sel = nonperson_imgs[:n_per_class]\n",
        "\n",
        "    def split_list(lst, val_ratio):\n",
        "        n_val = int(len(lst) * val_ratio)\n",
        "        return lst[n_val:], lst[:n_val]  # train, val\n",
        "\n",
        "    p_train, p_val = split_list(person_sel, val_ratio)\n",
        "    n_train, n_val = split_list(nonperson_sel, val_ratio)\n",
        "\n",
        "    def copy_files(files, dst_dir):\n",
        "        for f in files:\n",
        "            dst = dst_dir / f.name\n",
        "            # avoid rare collisions\n",
        "            if dst.exists():\n",
        "                dst = dst_dir / (f\"{f.parent.name}_{f.name}\")\n",
        "            shutil.copy2(f, dst)\n",
        "\n",
        "    print(\"üß© Creating VWW 10k subset...\")\n",
        "    copy_files(p_train, subset_dir / \"train\" / \"person\")\n",
        "    copy_files(p_val,   subset_dir / \"val\"   / \"person\")\n",
        "    copy_files(n_train, subset_dir / \"train\" / \"non_person\")\n",
        "    copy_files(n_val,   subset_dir / \"val\"   / \"non_person\")\n",
        "    print(\"‚úÖ VWW subset created at:\", subset_dir)\n",
        "\n",
        "download_vww()\n",
        "extract_vww()\n",
        "vww_root = find_vww_root()\n",
        "print(\"‚úÖ Found VWW root:\", vww_root)\n",
        "make_vww_subset(vww_root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx9VX0uXY7fa",
        "outputId": "33d2afdb-fa73-4a79-aa72-016e39c7add4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VWW archive already downloaded\n",
            "‚úÖ VWW already extracted\n",
            "‚úÖ Found VWW root: /content/vww_work/extracted/vw_coco2014_96\n",
            "‚úÖ VWW 10k subset already exists: /content/vww_work/vww_10k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Data Loaders (same style)\n",
        "# ----------------------------\n",
        "batch_size = 64\n",
        "img_size = 96\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(img_size, scale=(0.6, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=str(subset_dir / \"train\"), transform=train_transform)\n",
        "test_data  = datasets.ImageFolder(root=str(subset_dir / \"val\"),   transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Class mapping:\", train_data.class_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDoK0FE5Y2dL",
        "outputId": "986b0176-a8ea-466c-89fe-3d67985ea249"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class mapping: {'non_person': 0, 'person': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# CNN Model (same style)\n",
        "# ----------------------------\n",
        "class VWWConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # For 96x96 with 4 pools: 96->48->24->12->6\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)   # VWW: person vs non_person\n",
        "\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2) # 96 -> 48\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2) # 48 -> 24\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2) # 24 -> 12\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2) # 12 -> 6\n",
        "\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "OlsIIY59YzAv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Random Seeds and Model Instance (same style)\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(41)\n",
        "model = VWWConvNet().to(device)"
      ],
      "metadata": {
        "id": "S_kMARXuYuw3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Loss & Optimizer (same style)\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "FquLpcNNYtXq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training (same style)\n",
        "# ----------------------------\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train() # training mode\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(X)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad(set_to_none = True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        batch_size = y.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        preds = outputs.argmax(dim = 1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += batch_size\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "ejSwpoccYrqA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Testing (same style)\n",
        "# ----------------------------\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval() # testing mode\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Statistics\n",
        "            batch_size = y.size(0)\n",
        "            running_loss += loss.item() * batch_size\n",
        "            preds = outputs.argmax(dim = 1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += batch_size\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "gQuNVqf6YhcR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Run (same style print)\n",
        "# ----------------------------\n",
        "epochs = 30\n",
        "start_time = time.time()\n",
        "best_acc = 0\n",
        "best_epoch = 1\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch}/{epochs} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc * 100:.2f}% | \"\n",
        "        f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc * 100:.2f}%\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "\n",
        "print(f\"Training Time: {(time.time() - start_time) / 60} minutes!\")\n",
        "\n",
        "if best_acc > 0:\n",
        "  torch.save(model.state_dict(), \"/content/drive/My Drive/Colab Notebooks/stm_vww_best.pth\")\n",
        "  print(\"‚úÖ Model saved as stm_vww_model.pth for epoch\", best_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJE8_D_lYeQU",
        "outputId": "216f33de-6337-4338-e7c0-526328c7d035"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/30 | Train Loss: 0.6811 | Train Acc: 60.25% | Test Loss: 0.6614 | Test Acc: 58.90%\n",
            "Epoch: 2/30 | Train Loss: 0.6284 | Train Acc: 65.01% | Test Loss: 0.6052 | Test Acc: 67.60%\n",
            "Epoch: 3/30 | Train Loss: 0.6065 | Train Acc: 68.23% | Test Loss: 0.5901 | Test Acc: 68.60%\n",
            "Epoch: 4/30 | Train Loss: 0.5906 | Train Acc: 69.30% | Test Loss: 0.6126 | Test Acc: 65.95%\n",
            "Epoch: 5/30 | Train Loss: 0.5813 | Train Acc: 70.34% | Test Loss: 0.7081 | Test Acc: 61.35%\n",
            "Epoch: 6/30 | Train Loss: 0.5719 | Train Acc: 71.21% | Test Loss: 0.5887 | Test Acc: 68.80%\n",
            "Epoch: 7/30 | Train Loss: 0.5632 | Train Acc: 71.58% | Test Loss: 0.5499 | Test Acc: 71.25%\n",
            "Epoch: 8/30 | Train Loss: 0.5534 | Train Acc: 72.51% | Test Loss: 0.5593 | Test Acc: 70.80%\n",
            "Epoch: 9/30 | Train Loss: 0.5468 | Train Acc: 72.84% | Test Loss: 0.5993 | Test Acc: 68.75%\n",
            "Epoch: 10/30 | Train Loss: 0.5389 | Train Acc: 73.70% | Test Loss: 0.5457 | Test Acc: 72.60%\n",
            "Epoch: 11/30 | Train Loss: 0.5313 | Train Acc: 73.96% | Test Loss: 0.5982 | Test Acc: 70.80%\n",
            "Epoch: 12/30 | Train Loss: 0.5289 | Train Acc: 74.45% | Test Loss: 0.5342 | Test Acc: 74.55%\n",
            "Epoch: 13/30 | Train Loss: 0.5242 | Train Acc: 74.74% | Test Loss: 0.5417 | Test Acc: 72.95%\n",
            "Epoch: 14/30 | Train Loss: 0.5187 | Train Acc: 74.92% | Test Loss: 0.5303 | Test Acc: 74.10%\n",
            "Epoch: 15/30 | Train Loss: 0.5104 | Train Acc: 75.25% | Test Loss: 0.6027 | Test Acc: 69.35%\n",
            "Epoch: 16/30 | Train Loss: 0.5145 | Train Acc: 74.76% | Test Loss: 0.6444 | Test Acc: 67.70%\n",
            "Epoch: 17/30 | Train Loss: 0.5065 | Train Acc: 75.01% | Test Loss: 0.5112 | Test Acc: 75.55%\n",
            "Epoch: 18/30 | Train Loss: 0.4980 | Train Acc: 75.90% | Test Loss: 0.6011 | Test Acc: 71.25%\n",
            "Epoch: 19/30 | Train Loss: 0.4891 | Train Acc: 76.58% | Test Loss: 0.5018 | Test Acc: 77.40%\n",
            "Epoch: 20/30 | Train Loss: 0.4937 | Train Acc: 76.02% | Test Loss: 0.5479 | Test Acc: 72.10%\n",
            "Epoch: 21/30 | Train Loss: 0.4796 | Train Acc: 77.72% | Test Loss: 0.4955 | Test Acc: 76.40%\n",
            "Epoch: 22/30 | Train Loss: 0.4856 | Train Acc: 77.36% | Test Loss: 0.5194 | Test Acc: 72.65%\n",
            "Epoch: 23/30 | Train Loss: 0.4698 | Train Acc: 77.68% | Test Loss: 0.6800 | Test Acc: 65.45%\n",
            "Epoch: 24/30 | Train Loss: 0.4685 | Train Acc: 77.70% | Test Loss: 0.5248 | Test Acc: 74.20%\n",
            "Epoch: 25/30 | Train Loss: 0.4755 | Train Acc: 77.28% | Test Loss: 0.5506 | Test Acc: 74.45%\n",
            "Epoch: 26/30 | Train Loss: 0.4546 | Train Acc: 78.92% | Test Loss: 0.4802 | Test Acc: 77.40%\n",
            "Epoch: 27/30 | Train Loss: 0.4617 | Train Acc: 78.51% | Test Loss: 0.5197 | Test Acc: 74.45%\n",
            "Epoch: 28/30 | Train Loss: 0.4504 | Train Acc: 78.92% | Test Loss: 0.5630 | Test Acc: 70.60%\n",
            "Epoch: 29/30 | Train Loss: 0.4519 | Train Acc: 78.89% | Test Loss: 0.5008 | Test Acc: 74.60%\n",
            "Epoch: 30/30 | Train Loss: 0.4447 | Train Acc: 79.88% | Test Loss: 0.4963 | Test Acc: 75.70%\n",
            "Training Time: 8.67160218556722 minutes!\n",
            "‚úÖ Model saved as stm_vww_model.pth for epoch 19\n"
          ]
        }
      ]
    }
  ]
}