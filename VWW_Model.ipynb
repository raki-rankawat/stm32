{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0Pz6qNnYWO7gQWibv1BaV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raki-rankawat/stm32/blob/main/VWW_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import tarfile\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "XKrw7vV6YYI4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d1Medmla-18",
        "outputId": "531807ca-d5ae-4c44-83ef-4d0f7837430d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Auto Download + Prepare VWW (10k subset)\n",
        "# ----------------------------\n",
        "vww_url = \"https://www.silabs.com/public/files/github/machine_learning/benchmarks/datasets/vw_coco2014_96.tar.gz\"\n",
        "\n",
        "base_dir = Path(\"/content/vww_work\")\n",
        "archive_path = base_dir / \"vw_coco2014_96.tar.gz\"\n",
        "extract_dir = base_dir / \"extracted\"\n",
        "subset_dir = base_dir / \"vww_10k\"\n",
        "\n",
        "# Subset config: 5k person + 5k non_person\n",
        "n_per_class = 5000\n",
        "val_ratio = 0.20\n",
        "\n",
        "random.seed(41)\n",
        "torch.manual_seed(41)\n",
        "\n",
        "def download_vww():\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if archive_path.exists() and archive_path.stat().st_size > 0:\n",
        "        print(\"‚úÖ VWW archive already downloaded\")\n",
        "        return\n",
        "\n",
        "    print(\"‚¨áÔ∏è Downloading VWW archive...\")\n",
        "    urlretrieve(vww_url, archive_path)\n",
        "    print(\"‚úÖ Download complete:\", archive_path)\n",
        "\n",
        "def extract_vww():\n",
        "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if any(extract_dir.iterdir()):\n",
        "        print(\"‚úÖ VWW already extracted\")\n",
        "        return\n",
        "\n",
        "    print(\"üì¶ Extracting VWW archive...\")\n",
        "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
        "        tar.extractall(extract_dir)\n",
        "    print(\"‚úÖ Extraction complete:\", extract_dir)\n",
        "\n",
        "def find_vww_root():\n",
        "    # Find folder that contains BOTH person/ and non_person/\n",
        "    for p in extract_dir.rglob(\"person\"):\n",
        "        if p.is_dir() and (p.parent / \"non_person\").is_dir():\n",
        "            return p.parent\n",
        "    raise RuntimeError(\"‚ùå Could not find 'person' and 'non_person' directories under extracted dataset\")\n",
        "\n",
        "def list_images(folder):\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\"}\n",
        "    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
        "\n",
        "def make_vww_subset(src_root):\n",
        "    # Skip if subset already exists\n",
        "    if (subset_dir / \"train\" / \"person\").is_dir() and (subset_dir / \"val\" / \"non_person\").is_dir():\n",
        "        print(\"‚úÖ VWW 10k subset already exists:\", subset_dir)\n",
        "        return\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        for c in [\"person\", \"non_person\"]:\n",
        "            (subset_dir / split / c).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    person_imgs = list_images(src_root / \"person\")\n",
        "    nonperson_imgs = list_images(src_root / \"non_person\")\n",
        "\n",
        "    if len(person_imgs) < n_per_class or len(nonperson_imgs) < n_per_class:\n",
        "        raise ValueError(\n",
        "            f\"‚ùå Not enough images:\\n\"\n",
        "            f\"person: {len(person_imgs)} (need {n_per_class})\\n\"\n",
        "            f\"non_person: {len(nonperson_imgs)} (need {n_per_class})\"\n",
        "        )\n",
        "\n",
        "    random.shuffle(person_imgs)\n",
        "    random.shuffle(nonperson_imgs)\n",
        "\n",
        "    person_sel = person_imgs[:n_per_class]\n",
        "    nonperson_sel = nonperson_imgs[:n_per_class]\n",
        "\n",
        "    def split_list(lst, val_ratio):\n",
        "        n_val = int(len(lst) * val_ratio)\n",
        "        return lst[n_val:], lst[:n_val]  # train, val\n",
        "\n",
        "    p_train, p_val = split_list(person_sel, val_ratio)\n",
        "    n_train, n_val = split_list(nonperson_sel, val_ratio)\n",
        "\n",
        "    def copy_files(files, dst_dir):\n",
        "        for f in files:\n",
        "            dst = dst_dir / f.name\n",
        "            # avoid rare collisions\n",
        "            if dst.exists():\n",
        "                dst = dst_dir / (f\"{f.parent.name}_{f.name}\")\n",
        "            shutil.copy2(f, dst)\n",
        "\n",
        "    print(\"üß© Creating VWW 10k subset...\")\n",
        "    copy_files(p_train, subset_dir / \"train\" / \"person\")\n",
        "    copy_files(p_val,   subset_dir / \"val\"   / \"person\")\n",
        "    copy_files(n_train, subset_dir / \"train\" / \"non_person\")\n",
        "    copy_files(n_val,   subset_dir / \"val\"   / \"non_person\")\n",
        "    print(\"‚úÖ VWW subset created at:\", subset_dir)\n",
        "\n",
        "download_vww()\n",
        "extract_vww()\n",
        "vww_root = find_vww_root()\n",
        "print(\"‚úÖ Found VWW root:\", vww_root)\n",
        "make_vww_subset(vww_root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx9VX0uXY7fa",
        "outputId": "5a363b73-7075-4c4c-d2a6-530f758bfeab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VWW archive already downloaded\n",
            "‚úÖ VWW already extracted\n",
            "‚úÖ Found VWW root: /content/vww_work/extracted/vw_coco2014_96\n",
            "‚úÖ VWW 10k subset already exists: /content/vww_work/vww_10k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Data Loaders (same style)\n",
        "# ----------------------------\n",
        "batch_size = 64\n",
        "img_size = 96\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(img_size, scale=(0.6, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=str(subset_dir / \"train\"), transform=train_transform)\n",
        "test_data  = datasets.ImageFolder(root=str(subset_dir / \"val\"),   transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Class mapping:\", train_data.class_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDoK0FE5Y2dL",
        "outputId": "4f19c158-9ad3-487a-bf99-f693b38a6262"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class mapping: {'non_person': 0, 'person': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# CNN Model (same style) - FINAL\n",
        "# ----------------------------\n",
        "class VWWConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # For 96x96 with 4 pools: 96->48->24->12->6\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "OlsIIY59YzAv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Random Seeds and Model Instance (same style)\n",
        "# ----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(41)\n",
        "model = VWWConvNet().to(device)"
      ],
      "metadata": {
        "id": "S_kMARXuYuw3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Loss & Optimizer (same style) - FINAL\n",
        "# ----------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "FquLpcNNYtXq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------------\n",
        "# Training (same style)\n",
        "# ----------------------------\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train() # training mode\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(X)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(outputs, y)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad(set_to_none = True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        batch_size = y.size(0)\n",
        "        running_loss += loss.item() * batch_size\n",
        "        preds = outputs.argmax(dim = 1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += batch_size\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "ejSwpoccYrqA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Testing (same style)\n",
        "# ----------------------------\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval() # testing mode\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(X)\n",
        "\n",
        "            # Loss\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Statistics\n",
        "            batch_size = y.size(0)\n",
        "            running_loss += loss.item() * batch_size\n",
        "            preds = outputs.argmax(dim = 1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += batch_size\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "gQuNVqf6YhcR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Run (same style print) - FINAL (best checkpoint + early stopping)\n",
        "# ----------------------------\n",
        "epochs = 50\n",
        "start_time = time.time()\n",
        "\n",
        "best_acc = 0.0\n",
        "best_epoch = 1\n",
        "patience = 8\n",
        "patience_counter = 0\n",
        "\n",
        "best_path = \"/content/drive/My Drive/Colab Notebooks/stm_vww_best.pth\"\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch}/{epochs} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc * 100:.2f}% | \"\n",
        "        f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc * 100:.2f}%\"\n",
        "    )\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(\"‚úÖ Model saved as stm_vww_best.pth for epoch\", best_epoch)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"üõë Early stopping at epoch\", epoch, \"| Best epoch:\", best_epoch, \"| Best Test Acc:\", f\"{best_acc*100:.2f}%\")\n",
        "        break\n",
        "\n",
        "print(f\"Training Time: {(time.time() - start_time) / 60} minutes!\")\n",
        "print(\"Best epoch:\", best_epoch, \"| Best Test Acc:\", f\"{best_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJE8_D_lYeQU",
        "outputId": "b3c86b75-e0d7-4536-be8a-ebdd3b672f80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50 | Train Loss: 0.6801 | Train Acc: 60.68% | Test Loss: 0.6533 | Test Acc: 61.90%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 1\n",
            "Epoch: 2/50 | Train Loss: 0.6300 | Train Acc: 65.06% | Test Loss: 0.6269 | Test Acc: 66.00%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 2\n",
            "Epoch: 3/50 | Train Loss: 0.6066 | Train Acc: 68.25% | Test Loss: 0.6226 | Test Acc: 65.70%\n",
            "Epoch: 4/50 | Train Loss: 0.5873 | Train Acc: 69.11% | Test Loss: 0.6752 | Test Acc: 62.75%\n",
            "Epoch: 5/50 | Train Loss: 0.5746 | Train Acc: 70.08% | Test Loss: 0.6070 | Test Acc: 66.80%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 5\n",
            "Epoch: 6/50 | Train Loss: 0.5743 | Train Acc: 70.17% | Test Loss: 0.6373 | Test Acc: 64.65%\n",
            "Epoch: 7/50 | Train Loss: 0.5601 | Train Acc: 71.11% | Test Loss: 0.5850 | Test Acc: 68.90%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 7\n",
            "Epoch: 8/50 | Train Loss: 0.5582 | Train Acc: 71.97% | Test Loss: 0.5912 | Test Acc: 69.10%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 8\n",
            "Epoch: 9/50 | Train Loss: 0.5491 | Train Acc: 72.36% | Test Loss: 0.6423 | Test Acc: 68.95%\n",
            "Epoch: 10/50 | Train Loss: 0.5400 | Train Acc: 73.42% | Test Loss: 0.6246 | Test Acc: 66.25%\n",
            "Epoch: 11/50 | Train Loss: 0.5387 | Train Acc: 72.92% | Test Loss: 0.5808 | Test Acc: 68.90%\n",
            "Epoch: 12/50 | Train Loss: 0.5323 | Train Acc: 73.81% | Test Loss: 0.5578 | Test Acc: 72.15%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 12\n",
            "Epoch: 13/50 | Train Loss: 0.5229 | Train Acc: 73.99% | Test Loss: 0.5410 | Test Acc: 71.85%\n",
            "Epoch: 14/50 | Train Loss: 0.5212 | Train Acc: 74.84% | Test Loss: 0.5892 | Test Acc: 69.25%\n",
            "Epoch: 15/50 | Train Loss: 0.5145 | Train Acc: 75.06% | Test Loss: 0.6185 | Test Acc: 67.20%\n",
            "Epoch: 16/50 | Train Loss: 0.5060 | Train Acc: 75.21% | Test Loss: 0.6565 | Test Acc: 66.85%\n",
            "Epoch: 17/50 | Train Loss: 0.5068 | Train Acc: 75.38% | Test Loss: 0.7534 | Test Acc: 61.00%\n",
            "Epoch: 18/50 | Train Loss: 0.5057 | Train Acc: 75.31% | Test Loss: 0.6470 | Test Acc: 66.15%\n",
            "Epoch: 19/50 | Train Loss: 0.4954 | Train Acc: 76.26% | Test Loss: 0.5900 | Test Acc: 72.15%\n",
            "Epoch: 20/50 | Train Loss: 0.4949 | Train Acc: 75.94% | Test Loss: 0.5237 | Test Acc: 74.45%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 20\n",
            "Epoch: 21/50 | Train Loss: 0.4871 | Train Acc: 76.70% | Test Loss: 0.5340 | Test Acc: 72.80%\n",
            "Epoch: 22/50 | Train Loss: 0.4800 | Train Acc: 76.90% | Test Loss: 0.6094 | Test Acc: 69.10%\n",
            "Epoch: 23/50 | Train Loss: 0.4766 | Train Acc: 77.40% | Test Loss: 0.5426 | Test Acc: 73.55%\n",
            "Epoch: 24/50 | Train Loss: 0.4825 | Train Acc: 76.88% | Test Loss: 0.5237 | Test Acc: 73.60%\n",
            "Epoch: 25/50 | Train Loss: 0.4675 | Train Acc: 77.31% | Test Loss: 0.5396 | Test Acc: 73.95%\n",
            "Epoch: 26/50 | Train Loss: 0.4705 | Train Acc: 77.81% | Test Loss: 0.5100 | Test Acc: 74.85%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 26\n",
            "Epoch: 27/50 | Train Loss: 0.4665 | Train Acc: 77.36% | Test Loss: 0.5190 | Test Acc: 73.65%\n",
            "Epoch: 28/50 | Train Loss: 0.4579 | Train Acc: 78.00% | Test Loss: 0.5202 | Test Acc: 75.05%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 28\n",
            "Epoch: 29/50 | Train Loss: 0.4595 | Train Acc: 77.85% | Test Loss: 0.5017 | Test Acc: 76.15%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 29\n",
            "Epoch: 30/50 | Train Loss: 0.4552 | Train Acc: 78.85% | Test Loss: 0.5479 | Test Acc: 73.50%\n",
            "Epoch: 31/50 | Train Loss: 0.4489 | Train Acc: 78.83% | Test Loss: 0.5131 | Test Acc: 76.25%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 31\n",
            "Epoch: 32/50 | Train Loss: 0.4386 | Train Acc: 79.27% | Test Loss: 0.5766 | Test Acc: 71.70%\n",
            "Epoch: 33/50 | Train Loss: 0.4421 | Train Acc: 78.92% | Test Loss: 0.4954 | Test Acc: 76.70%\n",
            "‚úÖ Model saved as stm_vww_best.pth for epoch 33\n",
            "Epoch: 34/50 | Train Loss: 0.4368 | Train Acc: 79.33% | Test Loss: 0.5151 | Test Acc: 75.80%\n",
            "Epoch: 35/50 | Train Loss: 0.4388 | Train Acc: 78.91% | Test Loss: 0.5245 | Test Acc: 75.60%\n",
            "Epoch: 36/50 | Train Loss: 0.4339 | Train Acc: 79.65% | Test Loss: 0.5011 | Test Acc: 75.00%\n",
            "Epoch: 37/50 | Train Loss: 0.4280 | Train Acc: 80.23% | Test Loss: 0.4989 | Test Acc: 76.00%\n",
            "Epoch: 38/50 | Train Loss: 0.4311 | Train Acc: 79.57% | Test Loss: 0.5050 | Test Acc: 76.35%\n",
            "Epoch: 39/50 | Train Loss: 0.4283 | Train Acc: 79.80% | Test Loss: 0.5327 | Test Acc: 73.70%\n",
            "Epoch: 40/50 | Train Loss: 0.4288 | Train Acc: 79.97% | Test Loss: 0.5232 | Test Acc: 76.00%\n",
            "Epoch: 41/50 | Train Loss: 0.4204 | Train Acc: 80.19% | Test Loss: 0.5511 | Test Acc: 75.10%\n",
            "üõë Early stopping at epoch 41 | Best epoch: 33 | Best Test Acc: 76.70%\n",
            "Training Time: 11.08570119937261 minutes!\n",
            "Best epoch: 33 | Best Test Acc: 76.70%\n"
          ]
        }
      ]
    }
  ]
}