{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUV51zw2HEICjd/5WRz32d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrankawat/stm32/blob/main/Mnist_Pytorch_To_Int8_Onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "tnY_ymuM26b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install onnx onnxruntime onnxscript onnxruntime-tools"
      ],
      "metadata": {
        "id": "WKWZfTfskJXN",
        "collapsed": true
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TZfzVc-Nhw_7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ONNX + ORT quantization\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "from onnxruntime.quantization import quantize_dynamic, quantize_static, QuantType, QuantFormat, CalibrationDataReader"
      ],
      "metadata": {
        "id": "9ZN1aiFGiTTc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Data Loaders\n",
        "# -----------------------\n",
        "batch_size = 64\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_transform = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_transform = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_transform, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_transform, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Szl3CK8hMows"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Model\n",
        "# -----------------------\n",
        "class MNISTTinyCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 3, padding=1)\n",
        "    self.fc1 = nn.Linear(16 * 7 * 7, 196)\n",
        "    self.fc2 = nn.Linear(196, 49)\n",
        "    self.fc3 = nn.Linear(49, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, 2) # 28 / 2 -> 14\n",
        "\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, 2) # 14 / 2 -> 7\n",
        "\n",
        "    x = x.view(x.size(0), -1) # Flatten\n",
        "\n",
        "    x = F.relu(self.fc1(x)) # 784 -> 196\n",
        "    x = F.relu(self.fc2(x)) # 196 -> 49\n",
        "    x = self.fc3(x) # 49 -> 10 (logits)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "udq2NlbAlSPA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Random Seeds and Model Instance\n",
        "# -----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(41)\n",
        "model = MNISTTinyCNN().to(device)"
      ],
      "metadata": {
        "id": "xGLfldEsNKJk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Loss & Optimizer\n",
        "# -----------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hpRt8jInNLfm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Training\n",
        "# -----------------------\n",
        "def train(epoch, model, loader, criterion, optimizer):\n",
        "  model.train() # training mode\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  for b, (X, y) in enumerate(loader):\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Forward\n",
        "    outputs = model(X)\n",
        "\n",
        "    # Loss\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Backprop\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Metrics\n",
        "    batch_size = y.size(0)\n",
        "    running_loss += loss.item() * batch_size\n",
        "    preds = outputs.argmax(dim=1)\n",
        "    correct += (preds == y).sum().item()\n",
        "    total += batch_size\n",
        "\n",
        "  avg_loss = running_loss / total\n",
        "  accuracy = correct / total\n",
        "\n",
        "  return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "IaoMHa-ZownB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Testing\n",
        "# -----------------------\n",
        "def test(model, loader, criterion):\n",
        "  model.eval() # testing mode\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  running_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in loader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      # Forward\n",
        "      outputs = model(X)\n",
        "\n",
        "      # Loss\n",
        "      loss = criterion(outputs, y)\n",
        "\n",
        "      # Metrics\n",
        "      batch_size = y.size(0)\n",
        "      running_loss += loss.item() * batch_size\n",
        "      preds = outputs.argmax(dim=1)\n",
        "      correct += (preds == y).sum().item()\n",
        "      total += batch_size\n",
        "\n",
        "  avg_loss = running_loss / total\n",
        "  accuracy = correct / total\n",
        "\n",
        "  return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "hH4Pknv5iCGg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  train_loss, train_acc = train(epoch, model, train_loader, criterion, optimizer)\n",
        "  test_loss, test_acc = test(model, test_loader, criterion)\n",
        "\n",
        "  print(\n",
        "      f\"Epoch {epoch}/{epochs} | \"\n",
        "      f\"Train loss: {train_loss:.4f}, Train acc: {train_acc*100:.2f}% | \"\n",
        "      f\"Test loss: {test_loss:.4f}, Test acc: {test_acc*100:.2f}%\"\n",
        "  )\n",
        "\n",
        "print(f\"Time taken: {(time.time() - start_time) / 60} minutes!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq59cYsrOB0e",
        "outputId": "cb002fa3-723b-4b98-ae3c-b9c0b4dd2932"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train loss: 0.2322, Train acc: 93.04% | Test loss: 0.0706, Test acc: 97.73%\n",
            "Epoch 2/5 | Train loss: 0.0648, Train acc: 97.98% | Test loss: 0.0455, Test acc: 98.55%\n",
            "Epoch 3/5 | Train loss: 0.0451, Train acc: 98.59% | Test loss: 0.0325, Test acc: 98.84%\n",
            "Epoch 4/5 | Train loss: 0.0323, Train acc: 99.00% | Test loss: 0.0368, Test acc: 98.75%\n",
            "Epoch 5/5 | Train loss: 0.0264, Train acc: 99.15% | Test loss: 0.0389, Test acc: 98.88%\n",
            "Time taken: 2.8720115661621093 minutes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Compute Accuracy\n",
        "# -----------------------\n",
        "\n",
        "def compute_int8_accuracy(\n",
        "    labels_npz_path,\n",
        "    outputs_npz_path,\n",
        "    output_key=\"c_outputs_1\",\n",
        "    num_classes=10,\n",
        "    as_percentage=False\n",
        "):\n",
        "    labels = np.load(labels_npz_path)[\"label\"].astype(np.int64)\n",
        "    out = np.load(outputs_npz_path)\n",
        "\n",
        "    logits = out[output_key].reshape(len(labels), num_classes)\n",
        "    pred = np.argmax(logits, axis=1)\n",
        "\n",
        "    acc = (pred == labels).mean()\n",
        "    return acc * 100 if as_percentage else acc"
      ],
      "metadata": {
        "id": "xYLk1PYrCdI_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STM32 Pipeline"
      ],
      "metadata": {
        "id": "5Hy4b6w43Aaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Export To ONNX\n",
        "# -----------------------\n",
        "def export_onnx(model, onnx_path):\n",
        "  model.eval()\n",
        "\n",
        "  dummy = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "  torch.onnx.export(\n",
        "        model,\n",
        "        dummy,\n",
        "        onnx_path,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"logits\"],\n",
        "        export_params=True,\n",
        "        opset_version=18,\n",
        "        do_constant_folding=True,\n",
        "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
        "        dynamo=False\n",
        "    )\n",
        "  onnx.checker.check_model(onnx_path, full_check=False)\n",
        "  print(f\"ONNX model saved to: {onnx_path}\")\n",
        "\n",
        "export_onnx(model, \"mnist_lenet_fp32.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-gR8mIDPXq4",
        "outputId": "81f73457-74a1-4c66-a38e-229f19299904"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model saved to: mnist_lenet_fp32.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3970147135.py:9: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Load MNIST (NO normalization for now; matches what you already ran)\n",
        "# -----------------------\n",
        "test_loader_quant = DataLoader(test_transform, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "faD22MhmW8e5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Collect N samples\n",
        "# -----------------------\n",
        "N = 200\n",
        "inputs_nhwc = []\n",
        "logits = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (x, y) in enumerate(test_loader_quant):\n",
        "    model.eval()\n",
        "\n",
        "    if i >= N:\n",
        "      break\n",
        "\n",
        "    out = model(x)\n",
        "    # convert input to NHWC (1x28x28x1)\n",
        "    x_nhwc = x.numpy().transpose(0, 2, 3, 1).astype(np.float32)\n",
        "    inputs_nhwc.append(x_nhwc[0])\n",
        "    logits.append(out.numpy()[0].astype(np.float32))\n",
        "    labels.append(int(y.item()))\n",
        "\n",
        "inputs_nhwc = np.stack(inputs_nhwc, axis=0)\n",
        "logits = np.stack(logits, axis=0)\n",
        "labels = np.array(labels, dtype=np.int32)"
      ],
      "metadata": {
        "id": "8L_764WpXkbe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Save files\n",
        "# -----------------------\n",
        "# 1) Calibration file (for qantization later)\n",
        "np.savez(\"mnist_calib_200.npz\", input=inputs_nhwc)\n",
        "\n",
        "# 2) Validation file for stedgeai: input + ONE output array\n",
        "np.savez(\"mnist_val_200_io.npz\", input=inputs_nhwc, logits=logits)\n",
        "\n",
        "# 3) Labels saved separately (for accuracy in Python)\n",
        "np.savez(\"mnist_labels_200.npz\", label=labels)"
      ],
      "metadata": {
        "id": "JK5mhJTeY6q1"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run classifier for accuracy (Optional):\n",
        "\n",
        "```\n",
        "& \"C:\\Users\\rakes\\STM32Cube\\Repository\\Packs\\STMicroelectronics\\X-CUBE-AI\\10.2.0\\Utilities\\windows\\stedgeai.exe\" validate `\n",
        "  --target stm32n6 `\n",
        "  --name network `\n",
        "  -m C:\\Users\\rakes\\Downloads\\mnist_lenet_fp32.onnx `\n",
        "  --st-neural-art n6-allmems-O3@C:\\Users\\rakes\\STM32Cube\\Repository\\Packs\\STMicroelectronics\\X-CUBE-AI\\10.2.0\\scripts\\N6_scripts\\user_neuralart.json `\n",
        "  --workspace C:\\Users\\rakes\\AppData\\Local\\Temp\\mxAI_ws_val `\n",
        "  --output C:\\Users\\rakes\\.stm32cubemx\\network_output `\n",
        "  --mode target `\n",
        "  --valinput C:\\Users\\rakes\\Downloads\\mnist_val_200_io.npz `\n",
        "  --classifier `\n",
        "  --desc serial:COM3:921600\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd-0Bg1Duppb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = compute_int8_accuracy(\n",
        "    \"mnist_labels_200.npz\",\n",
        "    \"network_val_io.npz\",\n",
        "    as_percentage=True\n",
        ")\n",
        "\n",
        "print(\"STM32 INT8 accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Iyoh7y-CpgN",
        "outputId": "235e4cb7-d0ff-4968-a79d-8a35125fc823"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STM32 INT8 accuracy: 99.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Int8 Pipeline\n",
        "\n",
        "Because embedded hardware has brutal constraints:\n",
        "\n",
        "* Limited RAM\n",
        "* Limited flash\n",
        "* Power budget\n",
        "* Real-time deadlines\n",
        "\n",
        "INT8 is how we make DL fit and run fast on MCUs."
      ],
      "metadata": {
        "id": "xpjxqfrmwyEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTCalibReader(CalibrationDataReader):\n",
        "    def __init__(self, npz_path, input_name=\"input\"):\n",
        "        d = np.load(npz_path)\n",
        "        x = d[input_name].astype(np.float32)          # likely (N,28,28,1)\n",
        "\n",
        "        # If NHWC, convert to NCHW expected by your ONNX model\n",
        "        if x.ndim == 4 and x.shape[-1] == 1:         # (N,28,28,1)\n",
        "            x = np.transpose(x, (0, 3, 1, 2))         # -> (N,1,28,28)\n",
        "\n",
        "        self.x = x\n",
        "        self.input_name = input_name\n",
        "        self.i = 0\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.i >= len(self.x):\n",
        "            return None\n",
        "        batch = self.x[self.i:self.i+1]              # (1,1,28,28)\n",
        "        self.i += 1\n",
        "        return {self.input_name: batch}\n",
        "\n",
        "inp   = r\"mnist_lenet_fp32.onnx\"\n",
        "calib = r\"mnist_calib_200.npz\"\n",
        "out   = r\"mnist_lenet_int8_static_qdq.onnx\"\n",
        "\n",
        "reader = MNISTCalibReader(calib, input_name=\"input\")\n",
        "\n",
        "quantize_static(\n",
        "    model_input=inp,\n",
        "    model_output=out,\n",
        "    calibration_data_reader=reader,\n",
        "    quant_format=QuantFormat.QDQ,     # important for compiler friendliness\n",
        "    activation_type=QuantType.QInt8,\n",
        "    weight_type=QuantType.QInt8,\n",
        "    per_channel=True,\n",
        ")\n",
        "\n",
        "print(\"Saved:\", out)"
      ],
      "metadata": {
        "id": "vAu1WJhKuu1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10f2c6c-a495-4ed8-fbda-8eb9608c39ce"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: mnist_lenet_int8_static_qdq.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = compute_int8_accuracy(\n",
        "    \"mnist_labels_200.npz\",\n",
        "    \"network_val_io.npz\",\n",
        "    as_percentage=True\n",
        ")\n",
        "\n",
        "print(\"STM32 INT8 accuracy:\", acc)"
      ],
      "metadata": {
        "id": "MxSRmkXW3DDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9b00ba-3893-4146-ddd1-d9fcbb8eac30"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STM32 INT8 accuracy: 99.5\n"
          ]
        }
      ]
    }
  ]
}