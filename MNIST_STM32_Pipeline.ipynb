{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF6PsNzrBeAUrTXiH6afPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrankawat/stm32/blob/main/MNIST_STM32_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Trained MNIST Model"
      ],
      "metadata": {
        "id": "64fZMpKjrKIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1RJqSQDrpJY0"
      },
      "outputs": [],
      "source": [
        "!pip -q install onnx onnxruntime onnxscript onnxruntime-tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "from onnxruntime.quantization import quantize_dynamic, quantize_static, QuantType, QuantFormat, CalibrationDataReader"
      ],
      "metadata": {
        "id": "0HcZNlvopcM7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_hxHFUEpe2o",
        "outputId": "784908fd-60f7-4fe7-be84-0e1c372578d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loaders\n",
        "batch_size = 1\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "test_transform = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_transform, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "pE0wSm3BqO4U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class MNISTTinyCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 3, padding=1)\n",
        "    self.fc1 = nn.Linear(16 * 7 * 7, 196)\n",
        "    self.fc2 = nn.Linear(196, 49)\n",
        "    self.fc3 = nn.Linear(49, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, 2) # 28 / 2 -> 14\n",
        "\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, 2) # 14 / 2 -> 7\n",
        "\n",
        "    x = x.view(x.size(0), -1) # Flatten\n",
        "\n",
        "    x = F.relu(self.fc1(x)) # 784 -> 196\n",
        "    x = F.relu(self.fc2(x)) # 196 -> 49\n",
        "    x = self.fc3(x) # 49 -> 10 (logits)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Nd7BAyGRq_mM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load weights\n",
        "model = MNISTTinyCNN()\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/stm_mnist_model.pth\", map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRec17r7q4ds",
        "outputId": "1fa524dc-d4af-4d8d-903c-a831c5d562cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on the first 200 samples\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "        if i >= 200:\n",
        "            break\n",
        "        out = model(x)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(\"PyTorch accuracy on first 200:\", 100*correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju3CkaOMyxu5",
        "outputId": "540ab8c4-1532-48ea-97c3-8ce3fa0f0234"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch accuracy on first 200: 99.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FP32 Pipeline"
      ],
      "metadata": {
        "id": "CtbLjTI0rR9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export To ONNX\n",
        "def export_onnx(model, onnx_path):\n",
        "    model.eval()\n",
        "    dummy = torch.randn(1, 1, 28, 28)  # NCHW\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy,\n",
        "        onnx_path,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"logits\"],\n",
        "        export_params=True,\n",
        "        opset_version=18,\n",
        "        do_constant_folding=True,\n",
        "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
        "        dynamo=False\n",
        "    )\n",
        "    onnx.checker.check_model(onnx_path, full_check=False)\n",
        "    print(f\"ONNX model saved to: {onnx_path}\")\n",
        "\n",
        "export_onnx(model, \"mnist_lenet_fp32.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xVm8DEWrVAj",
        "outputId": "0d4a0987-c4ec-45f6-8fd7-0eed06e2580a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model saved to: mnist_lenet_fp32.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4171201524.py:6: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect N samples\n",
        "model.eval()\n",
        "\n",
        "N = 200\n",
        "inputs = []\n",
        "logits = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "        if i >= N:\n",
        "            break\n",
        "\n",
        "        out = model(x)\n",
        "\n",
        "        # Safe CPU conversion (works whether model is on CPU or CUDA)\n",
        "        x_nhwc = x.detach().cpu().numpy().transpose(0, 2, 3, 1).astype(np.float32)  # (1,28,28,1)\n",
        "        out_np = out.detach().cpu().numpy()[0].astype(np.float32)                   # (10,)\n",
        "\n",
        "        inputs.append(x_nhwc[0])   # (28,28,1)\n",
        "        logits.append(out_np)      # (10,)\n",
        "        labels.append(int(y.item()))\n",
        "\n",
        "inputs = np.stack(inputs, axis=0)          # (N,28,28,1)\n",
        "logits = np.stack(logits, axis=0)          # (N,10)\n",
        "labels = np.array(labels, dtype=np.int32)  # (N,)\n",
        "\n",
        "np.savez(\"mnist_val_200_io.npz\", input=inputs, logits=logits)\n",
        "np.savez(\"mnist_labels_200.npz\", label=labels)"
      ],
      "metadata": {
        "id": "Ha6JyMrsqtyB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute Accuracy\n",
        "def compute_accuracy(\n",
        "    labels_npz_path,\n",
        "    outputs_npz_path,\n",
        "    output_key=\"c_outputs_1\",\n",
        "    num_classes=10,\n",
        "    as_percentage=False\n",
        "):\n",
        "    labels = np.load(labels_npz_path)[\"label\"].astype(np.int64)\n",
        "    out = np.load(outputs_npz_path)\n",
        "\n",
        "    logits = out[output_key].reshape(len(labels), num_classes)\n",
        "    pred = np.argmax(logits, axis=1)\n",
        "\n",
        "    acc = (pred == labels).mean()\n",
        "    return acc * 100 if as_percentage else acc"
      ],
      "metadata": {
        "id": "Iw7j--kcu900"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = compute_accuracy(\n",
        "    \"mnist_labels_200.npz\",\n",
        "    \"network_val_io.npz\",\n",
        "    as_percentage=True\n",
        ")\n",
        "\n",
        "print(\"STM32 accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfVRCZeBu3X1",
        "outputId": "f5ad615c-9ce3-48cf-cec5-39e26b507132"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STM32 accuracy: 99.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Int8 Pipeline"
      ],
      "metadata": {
        "id": "yNpEHeCSvk7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibration NPZ (inputs only)\n",
        "def make_calib_npz(test_dataset, N=200, out_path=\"mnist_calib_200.npz\"):\n",
        "    loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    xs = []\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(loader):\n",
        "            if i >= N:\n",
        "                break\n",
        "            xs.append(x.detach().cpu().numpy()[0].astype(np.float32))  # (1,28,28)\n",
        "\n",
        "    xs = np.stack(xs, axis=0)  # (N,1,28,28)\n",
        "    np.savez(out_path, input=xs)\n",
        "    print(\"Saved calib:\", out_path, xs.shape)\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "BYJCOjeLvm5N"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize FP32 ONNX â†’ INT8 ONNX (QDQ)\n",
        "class CalibReader(CalibrationDataReader):\n",
        "    def __init__(self, npz_path, input_name=\"input\"):\n",
        "        self.x = np.load(npz_path)[\"input\"].astype(np.float32)  # (N,1,28,28)\n",
        "        self.input_name = input_name\n",
        "        self.i = 0\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.i >= len(self.x):\n",
        "            return None\n",
        "        batch = self.x[self.i:self.i+1]  # (1,1,28,28)\n",
        "        self.i += 1\n",
        "        return {self.input_name: batch}\n",
        "\n",
        "def quantize_int8_qdq(fp32_onnx=\"mnist_lenet_fp32.onnx\",\n",
        "                      calib_npz=\"mnist_calib_200.npz\",\n",
        "                      int8_onnx=\"mnist_lenet_int8_static_qdq.onnx\"):\n",
        "    reader = CalibReader(calib_npz, input_name=\"input\")\n",
        "\n",
        "    quantize_static(\n",
        "        model_input=fp32_onnx,\n",
        "        model_output=int8_onnx,\n",
        "        calibration_data_reader=reader,\n",
        "        quant_format=QuantFormat.QDQ,\n",
        "        activation_type=QuantType.QInt8,\n",
        "        weight_type=QuantType.QInt8,\n",
        "        per_channel=True,\n",
        "    )\n",
        "    print(\"Saved INT8:\", int8_onnx)\n",
        "    return int8_onnx"
      ],
      "metadata": {
        "id": "Wu04x83Vv_t3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calib_npz = make_calib_npz(test_transform, N=200, out_path=\"mnist_calib_200.npz\")\n",
        "quantize_int8_qdq(\"mnist_lenet_fp32.onnx\", calib_npz, \"mnist_lenet_int8_static_qdq.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "r6c7w4BfwHfK",
        "outputId": "cd6319cc-581c-4ccb-90c7-f9de7ce2dc12"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved calib: mnist_calib_200.npz (200, 1, 28, 28)\n",
            "Saved INT8: mnist_lenet_int8_static_qdq.onnx\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mnist_lenet_int8_static_qdq.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = compute_accuracy(\n",
        "    \"mnist_labels_200.npz\",\n",
        "    \"network_val_io.npz\",\n",
        "    as_percentage=True\n",
        ")\n",
        "\n",
        "print(\"STM32 accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhpJ6nd6xNYb",
        "outputId": "6566bb53-d01e-401e-dacd-764799ae4eb8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STM32 accuracy: 99.5\n"
          ]
        }
      ]
    }
  ]
}